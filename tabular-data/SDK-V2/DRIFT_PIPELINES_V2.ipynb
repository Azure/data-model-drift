{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Data and Model Drift Detection for Tabular Data\n",
        "\n",
        "The environment of our world is constantly changing. For machine learning, this means that deployed models are confronted with unknown data and can become outdated over time. A proactive drift management approach is required to ensure that productive AI services deliver consistent business value in the long term. Check out our background article [Getting traction on Data and Model Drift with Azure Machine Learning](https://medium.com/p/ebd240176b8b/edit) for an in-depth discussion about key concepts.\n",
        "\n",
        "This notebook provides the following mechanisms to detect and mitigate data and model drift:\n",
        "- Create automated pipelines to identify data drift regularly as part of an MLOps solution using Azure Machine Learning\n",
        "\n",
        "The notebook was developed and tested using the ``Python 3.8-AzureML`` kernel on a Azure ML Compute Instance."
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setup"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "from azure.identity import DefaultAzureCredential, InteractiveBrowserCredential\n",
        "from azure.ai.ml import MLClient, Input\n",
        "from azure.ai.ml.dsl import pipeline\n",
        "from azure.ai.ml import load_component\n",
        "\n",
        "# To access files better\n",
        "os.chdir(\"../\")\n",
        "print(os.getcwd())"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "/mnt/batch/tasks/shared/LS_root/mounts/clusters/natashasavic2/code/Users/natashasavic/data-model-drift/tabular-data\n"
        }
      ],
      "execution_count": 1,
      "metadata": {
        "gather": {
          "logged": 1650533885374
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "    credential = DefaultAzureCredential()\n",
        "    # Check if given credential can get token successfully.\n",
        "    credential.get_token(\"https://management.azure.com/.default\")\n",
        "except Exception as ex:\n",
        "    # Fall back to InteractiveBrowserCredential in case DefaultAzureCredential not work\n",
        "    credential = InteractiveBrowserCredential()"
      ],
      "outputs": [],
      "execution_count": 2,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# Get a handle to workspace\n",
        "ml_client = MLClient.from_config(credential=credential)\n",
        "\n",
        "# Retrieve an already attached Azure Machine Learning Compute.\n",
        "cluster_name = \"cpu-cluster\"\n",
        "# print(ml_client.compute.get(cluster_name))"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "Found the config file in: /mnt/batch/tasks/shared/LS_root/mounts/clusters/natashasavic2/code/Users/natashasavic/.azureml/config.json\n"
        }
      ],
      "execution_count": 3,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Build your custom environment\r\n",
        "\r\n",
        "Us this command to build the environment. If you need to make changes, make sure to update the environment and add a new version\r\n",
        "\r\n",
        "`az ml environment create --name data-model-drift-env --version 1 --file conda_image_docker.yml --conda-file conda_yamls/env_cli.yml --resource-group <add RG> --workspace-name <add workspace name>`\r\n"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Read input datasets\r\n",
        "\r\n",
        "For information on reading and writing data from / into different file storages (e.g. local, Azure Blob etc.), please refer to: https://docs.microsoft.com/en-us/azure/machine-learning/how-to-read-write-data-v2?tabs=CLI"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from azure.ai.ml import Input\r\n",
        "\r\n",
        "parent_dir = os.getcwd()\r\n",
        "\r\n",
        "# Retrieve files from a remote location such as the Blob storage\r\n",
        "pred_maintenance_input_remote = Input(\r\n",
        "    path=\"<Path to Blob>\",\r\n",
        "    type= \"uri_folder\"\r\n",
        ")\r\n",
        "\r\n",
        "# Retrieve files from location location \r\n",
        "pred_maintenance_input_local =  Input(\r\n",
        "      type=\"uri_folder\", \r\n",
        "      path = parent_dir + \"/data/data_raw/predictive_maintenance\")"
      ],
      "outputs": [],
      "execution_count": 4,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Define Azure ML Pipeline"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Paths to your custom defined components\n",
        "prep_yml = \"/SDK-V2/prep_data.yml\"\n",
        "drift_yml = \"/SDK-V2/data_drift.yml\"\n",
        "drift_db_yml = \"/SDK-V2/data_drift_db.yml\"\n",
        "\n",
        "\n",
        "# 1. Load components\n",
        "prepare_data = load_component(path=f\"{parent_dir}{prep_yml}\")\n",
        "measure_data_drift = load_component(f\"{parent_dir}{drift_yml}\")\n",
        "collect_data_drift_values = load_component(f\"{parent_dir}{drift_db_yml}\")\n",
        "\n",
        "\n",
        "# 2. Construct pipeline\n",
        "@pipeline()\n",
        "def data_drift_preprocess(pipeline_job_input):\n",
        "    # the parameters come from the respectove .yml file step. E.g. \"input_path\" is under inputs\n",
        "    transform_data = prepare_data(input_path=pipeline_job_input)\n",
        "    # the input for this pipeline is the output of the previous pipeline which is called \"output_path\"\n",
        "    drift_detect = measure_data_drift(\n",
        "        tansformed_data_path=transform_data.outputs.output_path,\n",
        "        threshold = 0.01\n",
        "    )\n",
        "    save_drift_db = collect_data_drift_values(\n",
        "        tansformed_data_path=transform_data.outputs.output_path,\n",
        "        threshold = 0.01\n",
        "    )\n",
        "    return {\n",
        "        \"pipeline_job_prepped_data\": transform_data.outputs.output_path,\n",
        "        \"pipeline_job_detect_data_drift\": drift_detect.outputs.drift_plot_path,\n",
        "        \"pipeline_job_store_data_drift\": save_drift_db.outputs.drift_db_path,\n",
        "\n",
        "    }\n",
        "\n",
        "# Define the input of your pipeline. In this example we only have one input which is the path to where the input data resides\n",
        "pipeline_job = data_drift_preprocess(pred_maintenance_input_local)\n",
        "\n",
        "\n",
        "# demo how to change pipeline output settings\n",
        "pipeline_job.outputs.pipeline_job_prepped_data.mode = \"upload\" # \"rw_mount\"\n",
        "pipeline_job.outputs.pipeline_job_detect_data_drift.mode = \"upload\" \n",
        "pipeline_job.outputs.pipeline_job_store_data_drift.mode = \"upload\" \n",
        "\n",
        "\n",
        "# set pipeline level compute\n",
        "pipeline_job.settings.default_compute=\"cpu-cluster\"\n",
        "# set pipeline level datastore\n",
        "pipeline_job.settings.default_datastore=\"workspaceblobstore\""
      ],
      "outputs": [],
      "execution_count": 5,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Submit pipeline\r\n",
        "\r\n",
        "Now you will submit your pipeline. To monitor its' status, please navigate to the `Jobs` pane on the left hand side of Azure ML"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# submit job to workspace\n",
        "experiment_name = \"data_drift_experiment\"\n",
        "\n",
        "pipeline_job = ml_client.jobs.create_or_update(\n",
        "    pipeline_job, experiment_name=experiment_name\n",
        ")\n",
        "pipeline_job"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "\r\u001b[32mUploading prep_src (0.0 MBs):   0%|          | 0/4392 [00:00<?, ?it/s]\r\u001b[32mUploading prep_src (0.0 MBs): 100%|██████████| 4392/4392 [00:00<00:00, 71122.01it/s]\n\u001b[39m\n\n"
        },
        {
          "output_type": "execute_result",
          "execution_count": 6,
          "data": {
            "text/plain": "PipelineJob({'inputs': {'pipeline_job_input': <azure.ai.ml.entities._job.pipeline._io.PipelineInput object at 0x7f11c9d8fcd0>}, 'outputs': {'pipeline_job_prepped_data': <azure.ai.ml.entities._job.pipeline._io.PipelineOutput object at 0x7f11c9d8fd00>, 'pipeline_job_detect_data_drift': <azure.ai.ml.entities._job.pipeline._io.PipelineOutput object at 0x7f11c9d8fd30>, 'pipeline_job_store_data_drift': <azure.ai.ml.entities._job.pipeline._io.PipelineOutput object at 0x7f11c9d8fd60>}, 'component': _PipelineComponent({'components': {}, 'auto_increment_version': False, 'is_anonymous': True, 'name': 'azureml_anonymous', 'description': None, 'tags': {}, 'properties': {}, 'id': None, 'source_path': None, 'base_path': None, 'creation_context': None, 'serialize': <msrest.serialization.Serializer object at 0x7f11c9d8f880>, 'version': '1', 'latest_version': None, 'schema': None, 'type': 'pipeline_component', 'display_name': 'data_drift_preprocess', 'is_deterministic': True, 'inputs': {'pipeline_job_input': {'type': 'unknown', 'mode': 'ro_mount'}}, 'outputs': {'pipeline_job_prepped_data': {'type': 'unknown', 'mode': 'rw_mount'}, 'pipeline_job_detect_data_drift': {'type': 'unknown', 'mode': 'rw_mount'}, 'pipeline_job_store_data_drift': {'type': 'unknown', 'mode': 'rw_mount'}}, 'source': 'REST', 'yaml_str': None, 'other_parameter': {}, 'func': <function [component] data_drift_preprocess at 0x7f11cad66af0>}), 'type': 'pipeline', 'status': 'Preparing', 'log_files': None, 'name': 'salmon_dream_kk6y4lrq1w', 'description': None, 'tags': {}, 'properties': {'mlflow.source.git.repoURL': 'git@github.com:natasha-savic-msft/data-model-drift.git', 'mlflow.source.git.branch': 'sdk_upgrade', 'mlflow.source.git.commit': '5f31f93e01584c142d6c422344b8e7459e8dbe01', 'azureml.git.dirty': 'True', 'azureml.DevPlatv2': 'true', 'azureml.runsource': 'azureml.PipelineRun', 'runSource': 'MFE', 'runType': 'HTTP', 'azureml.parameters': '{}', 'azureml.continue_on_step_failure': 'False', 'azureml.continue_on_failed_optional_input': 'True', 'azureml.pipelineComponent': 'pipelinerun'}, 'id': '/subscriptions/3a0dc8b1-97e1-4225-b97e-ab8bacd270f6/resourceGroups/NetworkWatcherRG/providers/Microsoft.MachineLearningServices/workspaces/Natasha_ANZ/jobs/salmon_dream_kk6y4lrq1w', 'source_path': None, 'base_path': '/mnt/batch/tasks/shared/LS_root/mounts/clusters/natashasavic2/code/Users/natashasavic/data-model-drift/tabular-data', 'creation_context': <azure.ai.ml._restclient.v2022_02_01_preview.models._models_py3.SystemData object at 0x7f11c9d8f8b0>, 'serialize': <msrest.serialization.Serializer object at 0x7f11c9d8fd90>, 'display_name': 'data_drift_preprocess', 'experiment_name': 'data_drift_experiment', 'compute': None, 'services': {'Tracking': <azure.ai.ml._restclient.v2022_02_01_preview.models._models_py3.JobService object at 0x7f11c9d8f550>, 'Studio': <azure.ai.ml._restclient.v2022_02_01_preview.models._models_py3.JobService object at 0x7f11c9d8f910>}, 'jobs': {'transform_data': Command({'parameters': {}, 'init': False, 'type': 'command', 'status': None, 'log_files': None, 'name': 'transform_data', 'description': None, 'tags': {}, 'properties': {}, 'id': None, 'source_path': None, 'base_path': '/mnt/batch/tasks/shared/LS_root/mounts/clusters/natashasavic2/code/Users/natashasavic/data-model-drift/tabular-data', 'creation_context': None, 'serialize': <msrest.serialization.Serializer object at 0x7f11c9d86e20>, 'allowed_keys': {}, 'key_restriction': False, 'logger': <Logger attr_dict (WARNING)>, 'display_name': None, 'experiment_name': None, 'compute': None, 'services': None, 'job_inputs': {'input_path': '${{parent.inputs.pipeline_job_input}}'}, 'job_outputs': {'output_path': '${{parent.outputs.pipeline_job_prepped_data}}'}, 'inputs': {'input_path': <azure.ai.ml.entities._job.pipeline._io.PipelineInputBase object at 0x7f11c9d86df0>}, 'outputs': {'output_path': <azure.ai.ml.entities._job.pipeline._io.PipelineOutputBase object at 0x7f11c9d86eb0>}, 'component': 'azureml_anonymous:dc3f1d7b-634d-4a97-a1a0-80d88f9e6ecc', 'kwargs': {}, 'instance_id': '95e62997-20f2-4297-9058-b8de6d4f08dc', 'limits': None, 'identity': None, 'distribution': None, 'environment_variables': {}, 'environment': None, 'resources': None, 'swept': False}), 'drift_detect': Command({'parameters': {}, 'init': False, 'type': 'command', 'status': None, 'log_files': None, 'name': 'drift_detect', 'description': None, 'tags': {}, 'properties': {}, 'id': None, 'source_path': None, 'base_path': '/mnt/batch/tasks/shared/LS_root/mounts/clusters/natashasavic2/code/Users/natashasavic/data-model-drift/tabular-data', 'creation_context': None, 'serialize': <msrest.serialization.Serializer object at 0x7f11c9d86bb0>, 'allowed_keys': {}, 'key_restriction': False, 'logger': <Logger attr_dict (WARNING)>, 'display_name': None, 'experiment_name': None, 'compute': None, 'services': None, 'job_inputs': {'threshold': '0.01', 'tansformed_data_path': '${{parent.jobs.transform_data.outputs.output_path}}'}, 'job_outputs': {'drift_plot_path': '${{parent.outputs.pipeline_job_detect_data_drift}}'}, 'inputs': {'threshold': <azure.ai.ml.entities._job.pipeline._io.PipelineInputBase object at 0x7f11c9d86d60>, 'tansformed_data_path': <azure.ai.ml.entities._job.pipeline._io.PipelineInputBase object at 0x7f11c9d8f7f0>}, 'outputs': {'drift_plot_path': <azure.ai.ml.entities._job.pipeline._io.PipelineOutputBase object at 0x7f11c9d8f820>}, 'component': 'azureml_anonymous:df112646-7b1e-465a-a3cd-45678c37e03d', 'kwargs': {}, 'instance_id': '9d57524b-63ee-41aa-9e8a-231f7d848ef7', 'limits': None, 'identity': None, 'distribution': None, 'environment_variables': {}, 'environment': None, 'resources': None, 'swept': False}), 'save_drift_db': Command({'parameters': {}, 'init': False, 'type': 'command', 'status': None, 'log_files': None, 'name': 'save_drift_db', 'description': None, 'tags': {}, 'properties': {}, 'id': None, 'source_path': None, 'base_path': '/mnt/batch/tasks/shared/LS_root/mounts/clusters/natashasavic2/code/Users/natashasavic/data-model-drift/tabular-data', 'creation_context': None, 'serialize': <msrest.serialization.Serializer object at 0x7f11c9d86a90>, 'allowed_keys': {}, 'key_restriction': False, 'logger': <Logger attr_dict (WARNING)>, 'display_name': None, 'experiment_name': None, 'compute': None, 'services': None, 'job_inputs': {'threshold': '0.01', 'tansformed_data_path': '${{parent.jobs.transform_data.outputs.output_path}}'}, 'job_outputs': {'drift_db_path': '${{parent.outputs.pipeline_job_store_data_drift}}'}, 'inputs': {'threshold': <azure.ai.ml.entities._job.pipeline._io.PipelineInputBase object at 0x7f11c9d8f790>, 'tansformed_data_path': <azure.ai.ml.entities._job.pipeline._io.PipelineInputBase object at 0x7f11c9d8f580>}, 'outputs': {'drift_db_path': <azure.ai.ml.entities._job.pipeline._io.PipelineOutputBase object at 0x7f11c9d8f250>}, 'component': 'azureml_anonymous:51c87eb0-b812-4cd3-bfc0-2ed1091ada3c', 'kwargs': {}, 'instance_id': '637b5ba7-703c-4f65-bb2b-f6fb1ff3c0d6', 'limits': None, 'identity': None, 'distribution': None, 'environment_variables': {}, 'environment': None, 'resources': None, 'swept': False})}, 'settings': <azure.ai.ml.entities._job.pipeline.pipeline_job_settings.PipelineJobSettings object at 0x7f11c9d8f670>, 'identity': None, 'schedule': None, 'default_code': None, 'default_environment': None, 'job_types': {'command': 3}, 'job_sources': {'REST': 3}})",
            "text/html": "<table style=\"width:100%\"><tr><th>Experiment</th><th>Name</th><th>Type</th><th>Status</th><th>Details Page</th></tr><tr><td>data_drift_experiment</td><td>salmon_dream_kk6y4lrq1w</td><td>pipeline</td><td>Preparing</td><td><a href=\"https://ml.azure.com/runs/salmon_dream_kk6y4lrq1w?wsid=/subscriptions/3a0dc8b1-97e1-4225-b97e-ab8bacd270f6/resourcegroups/NetworkWatcherRG/workspaces/Natasha_ANZ&amp;tid=72f988bf-86f1-41af-91ab-2d7cd011db47\" target=\"_blank\" rel=\"noopener\">Link to Azure Machine Learning studio</a></td></tr></table>"
          },
          "metadata": {}
        }
      ],
      "execution_count": 6,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Register component"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Re-load components to avoid \"reserved name\" error  \r\n",
        "\r\n",
        "parent_dir = os.getcwd()\r\n",
        "\r\n",
        "# Paths to your custom defined components\r\n",
        "prep_yml = \"/SDK-V2/prep_data.yml\"\r\n",
        "drift_yml = \"/SDK-V2/data_drift.yml\"\r\n",
        "drift_db_yml = \"/SDK-V2/data_drift_db.yml\"\r\n",
        "\r\n",
        "# 1. Load components\r\n",
        "prepare_data = load_component(path=f\"{parent_dir}{prep_yml}\")\r\n",
        "measure_data_drift = load_component(f\"{parent_dir}{drift_yml}\")\r\n",
        "collect_data_drift_values = load_component(f\"{parent_dir}{drift_db_yml}\")\r\n",
        "\r\n",
        "# Now we register the component to the workspace\r\n",
        "prepare_data_comp = ml_client.components.create_or_update(prepare_data)\r\n",
        "measure_data_drift_comp = ml_client.components.create_or_update(measure_data_drift)\r\n",
        "collect_data_drift_values_comp = ml_client.components.create_or_update(collect_data_drift_values)\r\n",
        "\r\n",
        "# Create (register) the component in your workspace\r\n",
        "print(\r\n",
        "    f\"Component {prepare_data.name} with Version {prepare_data.version} is registered\",\r\n",
        "    \"\\n\",\r\n",
        "    f\"Component {measure_data_drift.name} with Version {measure_data_drift.version} is registered\",\r\n",
        "    \"\\n\",\r\n",
        "    f\"Component {collect_data_drift_values.name} with Version {collect_data_drift_values.version} is registered\"\r\n",
        ")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Component prepare_drift_data with Version 1 is registered \n Component measure_data_drift with Version 1 is registered \n Component save_data_drift_values with Version 1 is registered\n"
        }
      ],
      "execution_count": 7,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Schedule Pipeline\r\n",
        "\r\n",
        "https://github.com/Azure/azureml-examples/blob/8a4070f55593c9641083784283b773f4f20955dd/sdk/jobs/pipelines/1f_pipeline_using_schedule/pipeline_using_schedule.ipynb"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# create a cron schedule start from current time and fire at minute 0,10 of every hour within the AEST TZ\r\n",
        "from datetime import datetime\r\n",
        "from dateutil import tz\r\n",
        "from azure.ai.ml.constants import TimeZone\r\n",
        "from azure.ai.ml.entities import (\r\n",
        "    CronSchedule,\r\n",
        "    RecurrenceSchedule,\r\n",
        "    RecurrencePattern,\r\n",
        "    ScheduleStatus,\r\n",
        ")\r\n",
        "\r\n",
        "schedule_start_time = datetime.now(tz=tz.gettz())\r\n",
        "cron_schedule = CronSchedule(\r\n",
        "    expression=\"0,10 * * * *\",\r\n",
        "    start_time=schedule_start_time,\r\n",
        "    time_zone=TimeZone.AUS_EASTERN_STANDARD_TIME,\r\n",
        "    status=ScheduleStatus.ENABLED,\r\n",
        ")\r\n",
        "# pipeline_job.schedule = cron_schedule"
      ],
      "outputs": [],
      "execution_count": 11,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Submit Jobs via CLI V2\r\n",
        "\r\n",
        "If you have an error \"the refresh token has expired\", use `az login` to athenticate in the CLI"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!conda activate azureml_py310_sdkv2\r\n",
        "!az ml  job create --file pipeline.yml"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    }
  ],
  "metadata": {
    "index_order": 5,
    "nbconvert_exporter": "python",
    "exclude_from_index": false,
    "pygments_lexer": "ipython3",
    "task": "Classification",
    "deployment": [
      "None"
    ],
    "authors": [
      {
        "name": "ratanase"
      }
    ],
    "vscode": {
      "interpreter": {
        "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
      }
    },
    "name": "python",
    "microsoft": {
      "host": {
        "AzureML": {
          "notebookHasBeenCompleted": true
        }
      }
    },
    "mimetype": "text/x-python",
    "kernel_info": {
      "name": "python38-azureml"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.5",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "compute": [
      "AML Compute"
    ],
    "version": "3.6.7",
    "kernelspec": {
      "name": "python38-azureml",
      "language": "python",
      "display_name": "Python 3.8 - AzureML"
    },
    "tags": [
      "remote_run",
      "AutomatedML"
    ],
    "datasets": [
      "Creditcard"
    ],
    "file_extension": ".py",
    "category": "tutorial",
    "framework": [
      "None"
    ],
    "friendly_name": "Classification of credit card fraudulent transactions using Automated ML",
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}