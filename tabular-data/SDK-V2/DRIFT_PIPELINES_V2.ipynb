{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Data and Model Drift Detection for Tabular Data\n",
        "\n",
        "The environment of our world is constantly changing. For machine learning, this means that deployed models are confronted with unknown data and can become outdated over time. A proactive drift management approach is required to ensure that productive AI services deliver consistent business value in the long term. Check out our background article [Getting traction on Data and Model Drift with Azure Machine Learning](https://medium.com/p/ebd240176b8b/edit) for an in-depth discussion about key concepts.\n",
        "\n",
        "This notebook provides the following mechanisms to detect and mitigate data and model drift:\n",
        "- Create automated pipelines to identify data drift regularly as part of an MLOps solution using Azure Machine Learning\n",
        "\n",
        "The notebook was developed and tested using the ``Python 3.8-AzureML`` kernel on a Azure ML Compute Instance."
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setup"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "from azure.identity import DefaultAzureCredential, InteractiveBrowserCredential\n",
        "from azure.ai.ml import MLClient, Input\n",
        "from azure.ai.ml.dsl import pipeline\n",
        "from azure.ai.ml import load_component\n",
        "\n",
        "# To access files better\n",
        "os.chdir(\"../\")\n",
        "print(os.getcwd())"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "/mnt/batch/tasks/shared/LS_root/mounts/clusters/natashasavic2/code/Users/natashasavic/data-model-drift/tabular-data\n"
        }
      ],
      "execution_count": 1,
      "metadata": {
        "gather": {
          "logged": 1650533885374
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "    credential = DefaultAzureCredential()\n",
        "    # Check if given credential can get token successfully.\n",
        "    credential.get_token(\"https://management.azure.com/.default\")\n",
        "except Exception as ex:\n",
        "    # Fall back to InteractiveBrowserCredential in case DefaultAzureCredential not work\n",
        "    credential = InteractiveBrowserCredential()"
      ],
      "outputs": [],
      "execution_count": 2,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# Get a handle to workspace\n",
        "ml_client = MLClient.from_config(credential=credential)\n",
        "\n",
        "# Retrieve an already attached Azure Machine Learning Compute.\n",
        "cluster_name = \"cpu-cluster\"\n",
        "# print(ml_client.compute.get(cluster_name))"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "Found the config file in: /mnt/batch/tasks/shared/LS_root/mounts/clusters/natashasavic2/code/Users/natashasavic/.azureml/config.json\n"
        }
      ],
      "execution_count": 3,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Build your custom environment\r\n",
        "\r\n",
        "Us this command to build the environment. If you need to make changes, make sure to update the environment and add a new version\r\n",
        "\r\n",
        "`az ml environment create --name data-model-drift-env --version 1 --file conda_image_docker.yml --conda-file conda_yamls/env_cli.yml --resource-group <add RG> --workspace-name <add workspace name>`\r\n"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Define Azure ML Pipeline"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "parent_dir = os.getcwd()\n",
        "\n",
        "# Paths to your custom defined components\n",
        "prep_yml = \"/SDK-V2/prep_data.yml\"\n",
        "drift_yml = \"/SDK-V2/data_drift.yml\"\n",
        "drift_db_yml = \"/SDK-V2/data_drift_db.yml\"\n",
        "\n",
        "\n",
        "# 1. Load components\n",
        "prepare_data = load_component(path=f\"{parent_dir}{prep_yml}\")\n",
        "measure_data_drift = load_component(f\"{parent_dir}{drift_yml}\")\n",
        "collect_data_drift_values = load_component(f\"{parent_dir}{drift_db_yml}\")\n",
        "\n",
        "\n",
        "# 2. Construct pipeline\n",
        "@pipeline()\n",
        "def data_drift_preprocess(pipeline_job_input):\n",
        "    # the parameters come from the respectove .yml file step. E.g. \"input_path\" is under inputs\n",
        "    transform_data = prepare_data(input_path=pipeline_job_input)\n",
        "    # the input for this pipeline is the output of the previous pipeline which is called \"output_path\"\n",
        "    drift_detect = measure_data_drift(\n",
        "        tansformed_data_path=transform_data.outputs.output_path,\n",
        "        threshold = 0.01\n",
        "    )\n",
        "    save_drift_db = collect_data_drift_values(\n",
        "        tansformed_data_path=transform_data.outputs.output_path,\n",
        "        threshold = 0.01\n",
        "    )\n",
        "    return {\n",
        "        \"pipeline_job_prepped_data\": transform_data.outputs.output_path,\n",
        "        \"pipeline_job_detect_data_drift\": drift_detect.outputs.drift_plot_path,\n",
        "        \"pipeline_job_store_data_drift\": save_drift_db.outputs.drift_db_path,\n",
        "\n",
        "    }\n",
        "\n",
        "pipeline_job = data_drift_preprocess(\n",
        "    Input(type=\"uri_folder\", path=parent_dir + \"/data/data_raw/predictive_maintenance\")\n",
        ")\n",
        "# demo how to change pipeline output settings\n",
        "pipeline_job.outputs.pipeline_job_prepped_data.mode = \"upload\" # \"rw_mount\"\n",
        "pipeline_job.outputs.pipeline_job_detect_data_drift.mode = \"upload\" \n",
        "pipeline_job.outputs.pipeline_job_store_data_drift.mode = \"upload\" \n",
        "\n",
        "\n",
        "# set pipeline level compute\n",
        "pipeline_job.settings.default_compute=\"cpu-cluster\"\n",
        "# set pipeline level datastore\n",
        "pipeline_job.settings.default_datastore=\"workspaceblobstore\""
      ],
      "outputs": [],
      "execution_count": 4,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Submit pipeline\r\n",
        "\r\n",
        "Now you will submit your pipeline. To monitor its' status, please navigate to the `Jobs` pane on the left hand side of Azure ML"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# submit job to workspace\n",
        "experiment_name = \"data_drift_experiment\"\n",
        "\n",
        "pipeline_job = ml_client.jobs.create_or_update(\n",
        "    pipeline_job, experiment_name=experiment_name\n",
        ")\n",
        "pipeline_job"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "\r\u001b[32mUploading data_drift_db_src (0.01 MBs):   0%|          | 0/9649 [00:00<?, ?it/s]\r\u001b[32mUploading data_drift_db_src (0.01 MBs): 100%|██████████| 9649/9649 [00:00<00:00, 174970.45it/s]\n\u001b[39m\n\n"
        },
        {
          "output_type": "execute_result",
          "execution_count": 5,
          "data": {
            "text/plain": "PipelineJob({'inputs': {'pipeline_job_input': <azure.ai.ml.entities._job.pipeline._io.PipelineInput object at 0x7f65c19ca0a0>}, 'outputs': {'pipeline_job_prepped_data': <azure.ai.ml.entities._job.pipeline._io.PipelineOutput object at 0x7f65c19ca070>, 'pipeline_job_detect_data_drift': <azure.ai.ml.entities._job.pipeline._io.PipelineOutput object at 0x7f65c19ca040>, 'pipeline_job_store_data_drift': <azure.ai.ml.entities._job.pipeline._io.PipelineOutput object at 0x7f65c1a43100>}, 'component': _PipelineComponent({'components': {}, 'auto_increment_version': False, 'is_anonymous': True, 'name': 'azureml_anonymous', 'description': None, 'tags': {}, 'properties': {}, 'id': None, 'source_path': None, 'base_path': None, 'creation_context': None, 'serialize': <msrest.serialization.Serializer object at 0x7f65c1b24d60>, 'version': '1', 'latest_version': None, 'schema': None, 'type': 'pipeline_component', 'display_name': 'data_drift_preprocess', 'is_deterministic': True, 'inputs': {'pipeline_job_input': {'type': 'unknown', 'mode': 'ro_mount'}}, 'outputs': {'pipeline_job_prepped_data': {'type': 'unknown', 'mode': 'rw_mount'}, 'pipeline_job_detect_data_drift': {'type': 'unknown', 'mode': 'rw_mount'}, 'pipeline_job_store_data_drift': {'type': 'unknown', 'mode': 'rw_mount'}}, 'source': 'REST', 'yaml_str': None, 'other_parameter': {}, 'func': <function [component] data_drift_preprocess at 0x7f65c28c2820>}), 'type': 'pipeline', 'status': 'Preparing', 'log_files': None, 'name': 'dynamic_frog_pp4jpnrj39', 'description': None, 'tags': {}, 'properties': {'mlflow.source.git.repoURL': 'git@github.com:natasha-savic-msft/data-model-drift.git', 'mlflow.source.git.branch': 'sdk_upgrade', 'mlflow.source.git.commit': '204d32f9bb24eb207e4b5fdd8dc8ed699e741d3c', 'azureml.git.dirty': 'True', 'azureml.runsource': 'azureml.PipelineRun', 'runSource': 'MFE', 'runType': 'HTTP', 'azureml.parameters': '{}', 'azureml.continue_on_step_failure': 'False', 'azureml.continue_on_failed_optional_input': 'True', 'azureml.pipelineComponent': 'pipelinerun'}, 'id': '/subscriptions/3a0dc8b1-97e1-4225-b97e-ab8bacd270f6/resourceGroups/NetworkWatcherRG/providers/Microsoft.MachineLearningServices/workspaces/Natasha_ANZ/jobs/dynamic_frog_pp4jpnrj39', 'source_path': None, 'base_path': '/mnt/batch/tasks/shared/LS_root/mounts/clusters/natashasavic2/code/Users/natashasavic/data-model-drift/tabular-data', 'creation_context': <azure.ai.ml._restclient.v2022_02_01_preview.models._models_py3.SystemData object at 0x7f65c1b24160>, 'serialize': <msrest.serialization.Serializer object at 0x7f65c1a430a0>, 'display_name': 'data_drift_preprocess', 'experiment_name': 'data_drift_experiment', 'compute': None, 'services': {'Tracking': <azure.ai.ml._restclient.v2022_02_01_preview.models._models_py3.JobService object at 0x7f65c1b24dc0>, 'Studio': <azure.ai.ml._restclient.v2022_02_01_preview.models._models_py3.JobService object at 0x7f65c1b24fa0>}, 'jobs': {'transform_data': Command({'parameters': {}, 'init': False, 'type': 'command', 'status': None, 'log_files': None, 'name': 'transform_data', 'description': None, 'tags': {}, 'properties': {}, 'id': None, 'source_path': None, 'base_path': '/mnt/batch/tasks/shared/LS_root/mounts/clusters/natashasavic2/code/Users/natashasavic/data-model-drift/tabular-data', 'creation_context': None, 'serialize': <msrest.serialization.Serializer object at 0x7f65c28b1100>, 'allowed_keys': {}, 'key_restriction': False, 'logger': <Logger attr_dict (WARNING)>, 'display_name': None, 'experiment_name': None, 'compute': None, 'services': None, 'job_inputs': {'input_path': '${{parent.inputs.pipeline_job_input}}'}, 'job_outputs': {'output_path': '${{parent.outputs.pipeline_job_prepped_data}}'}, 'inputs': {'input_path': <azure.ai.ml.entities._job.pipeline._io.PipelineInputBase object at 0x7f65c2910370>}, 'outputs': {'output_path': <azure.ai.ml.entities._job.pipeline._io.PipelineOutputBase object at 0x7f65c2910610>}, 'component': 'azureml_anonymous:30cab8fa-fbe7-4077-8465-91f471d08a24', 'kwargs': {}, 'instance_id': '43c57059-c361-4fc0-8a77-b861ed3b942c', 'limits': None, 'identity': None, 'distribution': None, 'environment_variables': {}, 'environment': None, 'resources': None, 'swept': False}), 'drift_detect': Command({'parameters': {}, 'init': False, 'type': 'command', 'status': None, 'log_files': None, 'name': 'drift_detect', 'description': None, 'tags': {}, 'properties': {}, 'id': None, 'source_path': None, 'base_path': '/mnt/batch/tasks/shared/LS_root/mounts/clusters/natashasavic2/code/Users/natashasavic/data-model-drift/tabular-data', 'creation_context': None, 'serialize': <msrest.serialization.Serializer object at 0x7f65c28b1820>, 'allowed_keys': {}, 'key_restriction': False, 'logger': <Logger attr_dict (WARNING)>, 'display_name': None, 'experiment_name': None, 'compute': None, 'services': None, 'job_inputs': {'threshold': '0.01', 'tansformed_data_path': '${{parent.jobs.transform_data.outputs.output_path}}'}, 'job_outputs': {'drift_plot_path': '${{parent.outputs.pipeline_job_detect_data_drift}}'}, 'inputs': {'threshold': <azure.ai.ml.entities._job.pipeline._io.PipelineInputBase object at 0x7f65c1b24700>, 'tansformed_data_path': <azure.ai.ml.entities._job.pipeline._io.PipelineInputBase object at 0x7f65c1b24c40>}, 'outputs': {'drift_plot_path': <azure.ai.ml.entities._job.pipeline._io.PipelineOutputBase object at 0x7f65c1b24f40>}, 'component': 'azureml_anonymous:df112646-7b1e-465a-a3cd-45678c37e03d', 'kwargs': {}, 'instance_id': 'ccfe975d-15ad-45ab-89d5-921732381e76', 'limits': None, 'identity': None, 'distribution': None, 'environment_variables': {}, 'environment': None, 'resources': None, 'swept': False}), 'save_drift_db': Command({'parameters': {}, 'init': False, 'type': 'command', 'status': None, 'log_files': None, 'name': 'save_drift_db', 'description': None, 'tags': {}, 'properties': {}, 'id': None, 'source_path': None, 'base_path': '/mnt/batch/tasks/shared/LS_root/mounts/clusters/natashasavic2/code/Users/natashasavic/data-model-drift/tabular-data', 'creation_context': None, 'serialize': <msrest.serialization.Serializer object at 0x7f65c1b24850>, 'allowed_keys': {}, 'key_restriction': False, 'logger': <Logger attr_dict (WARNING)>, 'display_name': None, 'experiment_name': None, 'compute': None, 'services': None, 'job_inputs': {'threshold': '0.01', 'tansformed_data_path': '${{parent.jobs.transform_data.outputs.output_path}}'}, 'job_outputs': {'drift_db_path': '${{parent.outputs.pipeline_job_store_data_drift}}'}, 'inputs': {'threshold': <azure.ai.ml.entities._job.pipeline._io.PipelineInputBase object at 0x7f65c1b24b80>, 'tansformed_data_path': <azure.ai.ml.entities._job.pipeline._io.PipelineInputBase object at 0x7f65c1b24d30>}, 'outputs': {'drift_db_path': <azure.ai.ml.entities._job.pipeline._io.PipelineOutputBase object at 0x7f65c1b24250>}, 'component': 'azureml_anonymous:ba744f0f-cb86-423d-a2a7-3d7b962d7221', 'kwargs': {}, 'instance_id': '83f4da42-2c88-4e64-9e9e-c6b9ec8280fa', 'limits': None, 'identity': None, 'distribution': None, 'environment_variables': {}, 'environment': None, 'resources': None, 'swept': False})}, 'settings': <azure.ai.ml.entities._job.pipeline.pipeline_job_settings.PipelineJobSettings object at 0x7f65c1b24a90>, 'identity': None, 'schedule': None, 'default_code': None, 'default_environment': None, 'job_types': {'command': 3}, 'job_sources': {'REST': 3}})",
            "text/html": "<table style=\"width:100%\"><tr><th>Experiment</th><th>Name</th><th>Type</th><th>Status</th><th>Details Page</th></tr><tr><td>data_drift_experiment</td><td>dynamic_frog_pp4jpnrj39</td><td>pipeline</td><td>Preparing</td><td><a href=\"https://ml.azure.com/runs/dynamic_frog_pp4jpnrj39?wsid=/subscriptions/3a0dc8b1-97e1-4225-b97e-ab8bacd270f6/resourcegroups/NetworkWatcherRG/workspaces/Natasha_ANZ&amp;tid=72f988bf-86f1-41af-91ab-2d7cd011db47\" target=\"_blank\" rel=\"noopener\">Link to Azure Machine Learning studio</a></td></tr></table>"
          },
          "metadata": {}
        }
      ],
      "execution_count": 5,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Register component"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Now we register the component to the workspace\r\n",
        "train_component = ml_client.create_or_update(train_component)\r\n",
        "\r\n",
        "# Create (register) the component in your workspace\r\n",
        "print(\r\n",
        "    f\"Component {train_component.name} with Version {train_component.version} is registered\"\r\n",
        ")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    }
  ],
  "metadata": {
    "index_order": 5,
    "nbconvert_exporter": "python",
    "exclude_from_index": false,
    "pygments_lexer": "ipython3",
    "task": "Classification",
    "deployment": [
      "None"
    ],
    "authors": [
      {
        "name": "ratanase"
      }
    ],
    "vscode": {
      "interpreter": {
        "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
      }
    },
    "name": "python",
    "microsoft": {
      "host": {
        "AzureML": {
          "notebookHasBeenCompleted": true
        }
      }
    },
    "mimetype": "text/x-python",
    "kernel_info": {
      "name": "python38-azureml"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.5",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "compute": [
      "AML Compute"
    ],
    "version": "3.6.7",
    "kernelspec": {
      "name": "python38-azureml",
      "language": "python",
      "display_name": "Python 3.8 - AzureML"
    },
    "tags": [
      "remote_run",
      "AutomatedML"
    ],
    "datasets": [
      "Creditcard"
    ],
    "file_extension": ".py",
    "category": "tutorial",
    "framework": [
      "None"
    ],
    "friendly_name": "Classification of credit card fraudulent transactions using Automated ML",
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}